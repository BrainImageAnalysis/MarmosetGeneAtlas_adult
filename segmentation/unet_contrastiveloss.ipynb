{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd678b84-569f-4082-85cb-58d92f1bc85a",
   "metadata": {},
   "source": [
    "# An automated pipeline to create an atlas of in-situ hybridization gene expression data in the adult marmoset brain (ISBI 2023, Poon, C., et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a442d-ce7d-4c9a-ba61-5a7eefb85c6c",
   "metadata": {},
   "source": [
    "# UNet based semantic segmentation model with contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872d3fa-e73d-4357-89cd-bca451f2886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 20221027"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4358a-ec6e-4f7c-a89d-2b07f38e6c72",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c6a88-c045-4121-a273-fea55a2da077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms.functional as tf\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67554a77-9060-422a-aaee-7501da217c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3dc3f-093e-493b-8da3-8077a751e34d",
   "metadata": {},
   "source": [
    "### Encoder and decoder blocks of UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea70382-d50a-4ff7-ac6c-a1fcb1210827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  encoder and decoder from UNet\n",
    "\n",
    "def convbnrelu2(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "def convbnrelu2_T(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "class unet_encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):  #64\n",
    "        super(unet_encoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_features = init_features\n",
    "        \n",
    "        self.conv_down1 = convbnrelu2(in_channels, init_features)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # not sure if this is right    \n",
    "        self.conv_down2 = convbnrelu2(init_features, init_features*2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # not sure if this is right    \n",
    "        \n",
    "        self.bottleneck = convbnrelu2(init_features*2, init_features*4)  #fc per channel, kernel size=1, reduces params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.conv_down1(x)\n",
    "        \n",
    "        enc2 = self.conv_down2(self.maxpool1(enc1))\n",
    "        bottleneck = self.bottleneck(self.maxpool2(enc2))\n",
    "        \n",
    "        return bottleneck#, enc_list\n",
    "\n",
    "class unet_decoder(nn.Module):\n",
    "    def __init__(self, init_features=16, out_channels=1):  #64\n",
    "        super(unet_decoder, self).__init__()\n",
    "    \n",
    "        #self.convT2 = nn.ConvTranspose2d(init_features*4, init_features*2, kernel_size=2, stride=2, bias=True)\n",
    "        self.convT2 = convbnrelu2_T(init_features*4, init_features*2)\n",
    "        self.conv_up2 = convbnrelu2(init_features*2, init_features*2)\n",
    "        \n",
    "        \n",
    "        #self.convT1 = nn.ConvTranspose2d(init_features*2, init_features, kernel_size=2, stride=2, bias=True)\n",
    "        self.convT1 = convbnrelu2_T(init_features*2, init_features)\n",
    "        self.conv_up1 = convbnrelu2(init_features, init_features)   \n",
    "        \n",
    "        #init_features =16\n",
    "        #out_channels=1\n",
    "        self.final_layer = nn.Conv2d(init_features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, bottleneck):\n",
    "        \n",
    "        dec2 = self.convT2(bottleneck)\n",
    "        dec2 = self.conv_up2(dec2)\n",
    "        \n",
    "        dec1 = self.convT1(dec2)\n",
    "        dec1 = self.conv_up1(dec1)\n",
    "        \n",
    "        out = self.final_layer(dec1)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a307af-7fb7-4947-a022-38af108c6418",
   "metadata": {},
   "source": [
    "### Define contrastive transformations (see SimCLR paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99adcbb-9b35-4ac5-a947-1e26c2729ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations:\n",
    "    def __init__(self, flag, n_views=2):\n",
    "        if flag == 'augm':\n",
    "            self.base_transforms = augm_transforms\n",
    "        elif flag == 'noaugm':\n",
    "            self.base_transforms = noaugm_transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]\n",
    "\n",
    "    \n",
    "augm_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=9),\n",
    "    ]\n",
    ")\n",
    "\n",
    "noaugm_transforms = transforms.Compose(\n",
    "    [\n",
    "    ]\n",
    ")    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae551b-49d3-43f8-81a4-a8017e67d07c",
   "metadata": {},
   "source": [
    "### Dataset used for both supervised and contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1bbac-6876-43c2-9366-6a8a0855ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% class ishDataset, char edit\n",
    "class ishDataset_sim(Dataset):\n",
    "    def __init__(self, root_dir: str, img_suffix: str, label_suffix: str, img_type: str): \n",
    "        self.root_dir = root_dir\n",
    "        self.img_path = os.path.join(root_dir, img_suffix)\n",
    "        self.label_path = os.path.join(root_dir, label_suffix)\n",
    "        self.img_type = img_type\n",
    "        self.img_fn_list = sorted(glob.glob(self.img_path+'*.'+self.img_type))  \n",
    "        self.label_fn_list = sorted(glob.glob(self.label_path+'*.'+self.img_type)) \n",
    "        \n",
    "        self.img_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        self.fast = False\n",
    "        self.fast2 = True\n",
    "        self.tt = transforms.ToTensor()\n",
    "        \n",
    "        if self.fast2:\n",
    "            for idx in range(len(self.img_fn_list)):                \n",
    "                im = self.tt(Image.open(self.img_fn_list[idx]))\n",
    "                lab = self.tt(Image.open(self.label_fn_list[idx]))\n",
    "                \n",
    "                im, lab = self.transform(im, lab)\n",
    "                \n",
    "                self.img_list += [im]\n",
    "                self.label_list+= [lab]\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.img_fn_list)\n",
    "    \n",
    "    def transform(self, image, label): \n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            image, output_size=(400,400))  \n",
    "        image = tf.crop(image, i, j, h, w)\n",
    "        label = tf.crop(label, i, j, h, w)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.fast2: \n",
    "            image, label = self.img_list[idx], self.label_list[idx]\n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022bc2-4f97-41fd-b7c4-a0fcd921773d",
   "metadata": {},
   "source": [
    "### Dataset used for contrastive learning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa6749-d279-4347-bada-b5f47085f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ishDataset_c(Dataset):\n",
    "    def __init__(self, root_dir: str, img_type: str):  \n",
    "        self.img_path = root_dir\n",
    "        self.img_fn_list = sorted(glob.glob(self.img_path+'*.'+self.img_type))  \n",
    "        self.img_list = []\n",
    "\n",
    "        self.fast = True\n",
    "        self.tt = transforms.ToTensor()\n",
    "        \n",
    "        if self.fast:\n",
    "            for idx in range(len(self.img_fn_list)):\n",
    "                im = Image.open(self.img_fn_list[idx])\n",
    "                im = self.transform(im)\n",
    "                im = self.tt(im)\n",
    "                self.img_list += [im]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fn_list)\n",
    "    \n",
    "    def transform(self, image): \n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(image,scale=(0.9, 1.5), ratio=(0.9, 1.33))  \n",
    "        output_size=(400,400)\n",
    "        image = tf.resized_crop(image, i, j, h, w, output_size)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.fast:  \n",
    "            image = self.img_list[idx]\n",
    "\n",
    "        return image\n",
    "        \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad56a90-c8b1-419f-90df-d173984c5cdd",
   "metadata": {},
   "source": [
    "### Load datasets and visualize some samples from dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca4546-0f8e-4290-8e57-a07e62eb0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data is assumed to be organized in the following structure:\n",
    "\n",
    "├── root directory                        , ie root_dir\n",
    "│   ├── image directory (training)        , ie img_suffix\n",
    "│   ├── label directory (training)        , ie label_suffix\n",
    "│   ├── image directory (testing)         , ie img_suffix\n",
    "│   ├── label directory (testing)         , ie label_suffix\n",
    "│   ├── image only directory (training)   , used in trainc_data\n",
    "\n",
    "\n",
    "where the image directory has the path:\n",
    "/root_dir/img_suffix/\n",
    "\n",
    "and the label directory has the path:\n",
    "/root_dir/label_suffix/\n",
    "\n",
    "and the images only directory has the path:\n",
    "/root_dir/img_only\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "trainsc_data = ishDataset_sim(root_dir='/home/username/projectA/',\n",
    "                          img_suffix='images_train/',   \n",
    "                          label_suffix='labels_train/',\n",
    "                          ) \n",
    "\n",
    "test_data = ishDataset_sim(root_dir='/home/username/projectA/'',\n",
    "                          img_suffix='images_test/',  \n",
    "                          label_suffix='labels_test/',\n",
    "                          )  \n",
    "\n",
    "# trainc_data is the data used for contrastive learning, ie only images no labels\n",
    "trainc_data = ishDataset_c(root_dir='/home/username/projectA/img_only/')\n",
    "\n",
    "# we split the training data in a 7:3 train/validation split\n",
    "print('train_data size:',len(trainsc_data))\n",
    "trainsplit_size = int(0.7 * len(trainsc_data))\n",
    "valsplit_size = len(trainsc_data) - trainsplit_size\n",
    "print('training_size:', trainsplit_size, 'val_size:', valsplit_size)\n",
    "trainsplit_data, valsplit_data = random_split(trainsc_data, [trainsplit_size, valsplit_size])\n",
    "\n",
    "# dataloader for supervised and contrastive learning\n",
    "trainsc_dataloader = DataLoader(trainsplit_data, batch_size=8, shuffle=True, drop_last=True, num_workers=1)\n",
    "# dataloader for contrastive learning only\n",
    "trainc_dataloader = DataLoader(trainc_data, batch_size=8, shuffle=True, drop_last=True, num_workers=1)\n",
    "# validation and test dataloaders\n",
    "val_dataloader = DataLoader(valsplit_data, batch_size=8, shuffle=False, drop_last=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, drop_last=True)   # currently test_dataloader is not used\n",
    "\n",
    "train_dataloaders = {\"con\": trainc_dataloader, \"sup\": trainsc_dataloader}\n",
    "\n",
    "\n",
    "# visualize some samples from the dataloaders\n",
    "if True:\n",
    "    x, y = next(iter(trainsc_dataloader))\n",
    "    print(x[0].shape)  \n",
    "    print(y[0].shape)\n",
    "    for s in range(4):\n",
    "        plt.subplot(221)\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.subplot(222)\n",
    "        plt.imshow(np.transpose(y[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup labels')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(trainsc_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90351da-c16c-4d65-8b91-f9fc414011c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some samples from the dataloaders\n",
    "f True:\n",
    "    x, y = next(iter(val_dataloader))\n",
    "    print(x[0].shape)  # supx_s[0][0] if stay tensor\n",
    "    print(y[0].shape)\n",
    "    for s in range(4):\n",
    "        plt.subplot(221)\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.subplot(222)\n",
    "        plt.imshow(np.transpose(y[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup labels')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(val_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc5ab9-246f-47d4-9e6d-4233a39c6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some samples from the dataloaders\n",
    "if True:\n",
    "    x = next(iter(trainc_dataloader))\n",
    "    print(x[0].shape)  # supx_s[0][0] if stay tensor\n",
    "    for s in range(4):\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(trainc_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd016d07-18b0-471e-98ff-ec1188ddf123",
   "metadata": {},
   "source": [
    "### Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf7b4-5735-406f-aa39-773d9ee8ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, bottleneck_ch, bottleneck_h, bottleneck_w, lr, temperature, weight_decay, p_conloss):\n",
    "        super(SegModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lr = lr  #5e-4\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = 8\n",
    "        self.bottleneck_ch = bottleneck_ch\n",
    "        self.bottleneck_w = bottleneck_w\n",
    "        self.bottleneck_h = bottleneck_h\n",
    "        self.p_conloss = p_conloss\n",
    "        self.counter = 0\n",
    "        self.bce = torch.nn.BCELoss()  \n",
    "    \n",
    "        self.encoder = unet_encoder(in_channels=3, out_channels=1, init_features=16)  #64\n",
    "        self.decoder = unet_decoder(init_features=16, out_channels=1)  #64\n",
    "        \n",
    "        self.trainsc_data = trainsplit_data\n",
    "        self.trainc_data = trainc_data\n",
    "        self.valset = valsplit_data\n",
    "        self.testset = test_data\n",
    "        \n",
    "        self.ct_null = ContrastiveTransformations(flag='noaugm', n_views=2)\n",
    "        self.ct = ContrastiveTransformations(flag='augm', n_views=2)\n",
    "        \n",
    "        # conv2d for the MLP (below)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels=64,  #1024\n",
    "            out_channels=hidden_dim*4,\n",
    "            kernel_size=[bottleneck_w, bottleneck_h],\n",
    "            stride = [bottleneck_h, bottleneck_w]\n",
    "        )\n",
    "        # MLP used for the contrastive loss\n",
    "        self.mlp = nn.Sequential( \n",
    "            self.conv2d,\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64*hidden_dim, hidden_dim), \n",
    "            nn.Linear(hidden_dim,4*2)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        trainsc_dataloader = DataLoader(self.trainsc_data, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "        trainc_dataloader = DataLoader(self.trainc_data, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "        train_dataloaders = {\"con\": trainc_dataloader, \"sup\": trainsc_dataloader}\n",
    "        return train_dataloaders  \n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, batch_size=self.batch_size, shuffle=False, drop_last=True, num_workers=1)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        bottleneck = self.encoder(x)\n",
    "        preds = self.decoder(bottleneck)\n",
    "        return (bottleneck, preds)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) :  # , batch_nb\n",
    "        \n",
    "        if not self.automatic_optimization:\n",
    "            opt = self.optimizers()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            \n",
    "        img_train, mask_train = batch[\"sup\"] \n",
    "        img_nomi = batch[\"con\"]\n",
    "        \n",
    "        print('img_train len:', len(img_train))\n",
    "        print('mask_train len:', len(mask_train))\n",
    "\n",
    "\n",
    "        # INFO NCE LOSS #         \n",
    "\n",
    "        img_train_c = self.ct(img_train)\n",
    "        mask_train_c = self.ct_null(mask_train)\n",
    "        imgcat_train_c = torch.cat(img_train_c, dim=0)\n",
    "        bottleneck_train_c, preds_train_c = self.forward(imgcat_train_c)\n",
    "\n",
    "        loss_nce_train = self.info_nce_loss(bottleneck_train_c, mode='train')\n",
    "        self.log(\"train_contr_loss\", loss_nce_train, on_epoch=True)        \n",
    "        \n",
    "        img_nomi_c = self.ct(img_nomi)\n",
    "        imgcat_nomi_c = torch.cat(img_nomi_c, dim=0)\n",
    "        bottleneck_nomi_c, _= self.forward(imgcat_nomi_c)\n",
    "        loss_nce_nomi = self.info_nce_loss(bottleneck_nomi_c, mode='train')\n",
    "        self.log(\"train_only_contrastive\", loss_nce_nomi, on_epoch=True) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # SUPERVISED LOSS #\n",
    "       \n",
    "        mask_train2 = (mask_train > 0.5).to(dtype=torch.float32)\n",
    "        bottleneck_train_s, preds_train_s = self.forward(img_train)\n",
    "        loss_sup_train = self.bce(preds_train_s, mask_train2)\n",
    "        self.log(\"train_sup_loss\", loss_sup_train, on_epoch=True)\n",
    "\n",
    "        train_loss = loss_sup_train + (self.p_conloss/10)*loss_nce_train + (self.p_conloss/100)*loss_nce_nomi\n",
    "        \n",
    "        self.log(\"train_loss\", train_loss, on_epoch=True)\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        if not self.automatic_optimization:\n",
    "            print('if not self.automatic_optimization:')\n",
    "            self.manual_backward(loss_sup_train + 0.5*loss_nce_train)\n",
    "            opt.step()\n",
    "            \n",
    "        else:\n",
    "            return {'loss' : train_loss}\n",
    "    \n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):  # \n",
    "        if self.counter % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            x,y = batch         \n",
    "            \n",
    "\n",
    "            # INFO NCE LOSS #  \n",
    "\n",
    "            x_c = self.ct(x)\n",
    "            y_c = self.ct_null(y)\n",
    "            x_cat_c = torch.cat(x_c, dim=0)\n",
    "            y_cat_c = torch.cat(y_c, dim=0)\n",
    "\n",
    "            b_c, y_hat_c = self.forward(x_cat_c)\n",
    "            #print('b_c:',b_c.shape)\n",
    "            loss_c = self.info_nce_loss(b_c, mode='train')\n",
    "            loss_cs = self.bce(y_hat_c, y_cat_c)\n",
    "            self.log(\"val_supduringcont_loss\", loss_cs, on_epoch=True)\n",
    "\n",
    "            self.log(\"val_cont_loss\", loss_c, on_epoch=True)\n",
    "            print('val_step, nce loss:', loss_c.item())\n",
    "\n",
    "\n",
    "            # SUPERVISED LOSS #\n",
    "\n",
    "            y = (y > 0.5).to(dtype=torch.float32)\n",
    "\n",
    "            b_s, y_hat_s = self.forward(x)\n",
    "            loss_s = self.bce(y_hat_s, y)\n",
    "\n",
    "            self.log(\"val_sup_loss\", loss_s, on_epoch=True)\n",
    "            \n",
    "            # visualize validation outputs\n",
    "            if True:\n",
    "                cyhats = torch.cat((y_hat_c.detach().cpu()[0,0,...], y_hat_c.detach().cpu()[1,0,...],\n",
    "                                     y_hat_c.detach().cpu()[6,0,...], y_hat_c.detach().cpu()[7,0,...]), dim=1\n",
    "                                     )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cyhats, cmap='gray')\n",
    "                plt.title('contrastive: y_hats')\n",
    "                plt.show()\n",
    "\n",
    "                cx1 = torch.cat(\n",
    "                            (x_c[0].detach().cpu()[0,0,...], x_c[0].detach().cpu()[1,0,...],\n",
    "                              x_c[0].detach().cpu()[2,0,...], x_c[0].detach().cpu()[3,0,...]),dim=1   \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cx1, cmap='gray')\n",
    "                plt.title('contrastive: x')\n",
    "                plt.show()\n",
    "\n",
    "                cy = torch.cat(\n",
    "                            (y_c[0].detach().cpu()[0,0,...], y_c[0].detach().cpu()[1,0,...],\n",
    "                             y_c[0].detach().cpu()[2,0,...], y_c[0].detach().cpu()[3,0,...]),dim=1     \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cy, cmap='gray')\n",
    "                plt.title('contrastive: y')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                syhats = torch.cat((y_hat_s.detach().cpu()[0,0,...],  y_hat_s.detach().cpu()[1,0,...], \n",
    "                                  y_hat_s.detach().cpu()[2,0,...], y_hat_s.detach().cpu()[3,0,...]), dim=1  \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(syhats, cmap='gray')\n",
    "                plt.title('supervised: y_hats')\n",
    "                plt.show()\n",
    "\n",
    "                sx = torch.cat(\n",
    "                            (x.detach().cpu()[0,0,...], x.detach().cpu()[1,0,...],\n",
    "                             x.detach().cpu()[2,0,...], x.detach().cpu()[3,0,...]),dim=1)\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(sx, cmap='gray')\n",
    "                plt.title('supervised: x')\n",
    "                plt.show()\n",
    "\n",
    "                sy = torch.cat(\n",
    "                            (y.detach().cpu()[0,0,...], y.detach().cpu()[1,0,...],\n",
    "                             y.detach().cpu()[2,0,...], y.detach().cpu()[3,0,...]),dim=1 \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(sy, cmap='gray')\n",
    "                plt.title('supervised: y')\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "    def info_nce_loss(self, bottleneck, mode):\n",
    "        # from https://theaisummer.com/simclr/\n",
    "        \n",
    "        feats = self.mlp(bottleneck)\n",
    "        mask = (~torch.eye(self.batch_size * 2, self.batch_size * 2, dtype=bool, device=self.device)).float()\n",
    "\n",
    "        ai_cos = F.cosine_similarity(feats.unsqueeze(1), feats.unsqueeze(0), dim=2)\n",
    "\n",
    "        sim_ij = torch.diag(ai_cos, self.batch_size) \n",
    "        sim_ji = torch.diag(ai_cos, -self.batch_size)  \n",
    "\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        # nominator has positive pairs only\n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        # denominator has both positive and negative pairs, but mask each element from itself (inverse identity)\n",
    "        denominator = mask * torch.exp(ai_cos / self.temperature)\n",
    "       \n",
    "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        nce_loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "        print('nce_loss:', nce_loss)  # tensor(2.4993)\n",
    "        \n",
    "        self.log(mode + \"_nce_loss\", nce_loss)\n",
    "        \n",
    "        return nce_loss\n",
    "    \n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters())\n",
    "        return [opt]#, [sch]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a8cf8-8b93-4740-9e59-c0d6e3f6de40",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5936098-fb3b-4715-9a71-7968723cb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create name for saved model\n",
    "date = 20221027\n",
    "max_epochs = 100\n",
    "p_conloss = 1\n",
    "out_name=str(date)+'_pretraining_epochs'+str(max_epochs)+'_patch400'\n",
    "\n",
    "segmodel = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, temperature=.07, weight_decay=1e-4, p_conloss=1)  \n",
    "\n",
    "# initialize with pretrained model (optional)\n",
    "pretrain_dir = '/home/username/projectA/savedmodels/pretrainedmodel.ckpt'\n",
    "pre = torch.load(pretrain_dir)\n",
    "segmodel.load_state_dict(pre['state_dict'])\n",
    "\n",
    "# define checkpoint directory\n",
    "ckpt_dir = '/home/username/projectA/ckpt/'\n",
    "os.makedirs(ckpt_dir,exist_ok=True)\n",
    "\n",
    "if True:\n",
    "    print('training with if True:')\n",
    "    trainer = pl.Trainer(logger=CSVLogger(save_dir = 'csvlogs/', name=out_name),\n",
    "                         accelerator='gpu',\n",
    "                         gpus=1,\n",
    "                        devices='gpus', \n",
    "                        default_root_dir = ckpt_dir, \n",
    "                        enable_progress_bar=False, \n",
    "                        enable_model_summary=True, \n",
    "                        max_epochs=200)  \n",
    "\n",
    "    trainer.fit(segmodel)\n",
    "\n",
    "else:  # pure pytorch\n",
    "    optimizer = torch.optim.Adam(segmodel.parameters())\n",
    "    loss = torch.nn.BCELoss()\n",
    "    segmodel.cuda()\n",
    "    for a in range(10000+1):\n",
    "        patch = trainx_s[0].cuda()\n",
    "        patch_gt = trainy_s[0].cuda()\n",
    "        segmodel.zero_grad()\n",
    "        tmp,out_ = segmodel(patch)\n",
    "        prediction = torch.sigmoid(out_)\n",
    "        l = loss(prediction,patch_gt)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if True:\n",
    "            if a % 100 == 0:\n",
    "                print('a:',a)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"iter \",a,\" loss : \",l)\n",
    "                    segmodel.eval()\n",
    "                    tmp,out_ = segmodel(patch)\n",
    "                    prediction = torch.sigmoid(out_)\n",
    "                    rimg = torch.cat(\n",
    "                        (prediction.detach().cpu()[0,0,...],patch_gt.cpu()[0,0,...]),dim=1\n",
    "                    )\n",
    "                    plt.imshow(rimg)\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642f03f-15e0-48c4-b704-92e67c815da0",
   "metadata": {},
   "source": [
    "### Visualize some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc719d98-b4d6-4fbc-bb1f-441599ee3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [4.0, 4.0]\n",
    "plt.rcParams['figure.dpi'] = 70\n",
    "\n",
    "# load metrics using pd.read_csv if you would like to visualize metrics from a different run\n",
    "#metrics = pd.read_csv('/home/username/projectA/csvlogs/lightning_logs/version_x/metrics.csv')\n",
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "del metrics[\"step\"]\n",
    "\n",
    "del metrics[\"train_loss_step\"]  \n",
    "\n",
    "del metrics[\"train_loss_epoch\"]\n",
    "\n",
    "del metrics['train_nce_loss']\n",
    "del metrics['train_contr_loss_step']\n",
    "del metrics['train_sup_loss_step']\n",
    "\n",
    "\n",
    "metrics.set_index(\"epoch\", inplace=True)\n",
    "display(metrics.dropna(axis=1, how=\"all\").head())\n",
    "sn.relplot(data=metrics, kind=\"line\", height=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f125f82-f94f-4eb0-87e5-bc15ee36be16",
   "metadata": {},
   "source": [
    "### Run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1b83d-cb8e-44b4-bb56-7fe230532d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "\n",
    "# if loading model from file\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if True:\n",
    "    segmodel2 = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, temperature=.07, weight_decay=1e-4, p_conloss=1)  # preds_h,w = 6 for patch64,96, 3 for patch48\n",
    "    segmodel2.load_state_dict(torch.load('/home/username/projectA/savedmodels/segmodel_epoch=499-step=778500.ckpt')['state_dict'])  #ish_pth/20220219_unet3lvl_crop64_best_metric_model_91.pth'))\n",
    "    segmodel2.freeze()\n",
    "    segmodel2.eval()\n",
    "\n",
    "for cmp in cmp_ls: \n",
    "    test_dir = '/home/username/projectA/images_test/'\n",
    "    odir = test_dir.replace('images','outputs')\n",
    "    os.makedirs(odir, exist_ok=True) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(test_f)):\n",
    "            unique_name = os.path.split(test_f[j])[1]\n",
    "\n",
    "            nibobj = nib.load(test_f[j])\n",
    "            im = nibobj.get_fdata()\n",
    "\n",
    "            im_t = tf.to_tensor(im)  \n",
    "            im_t2 = torch.unsqueeze(im_t,0)\n",
    "            _, pred = segmodel2(im_t2.float())\n",
    "\n",
    "            pred_ = pred[0,0,:,:].numpy()\n",
    "            im_out2 = rescale_intensity(pred_, in_range='image',out_range='uint8').astype(np.uint8) \n",
    "\n",
    "            print(out_name)\n",
    "            \n",
    "            new_im = nib.Nifti1Image(im_out2, nibobj.affine, nibobj.header)\n",
    "            nib.save(new_im, out_name)\n",
    "\n",
    "            if j % 10 == 0:\n",
    "                plt.imshow(im_out2, cmap='gray')\n",
    "                plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterhub] *",
   "language": "python",
   "name": "conda-env-jupyterhub-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
