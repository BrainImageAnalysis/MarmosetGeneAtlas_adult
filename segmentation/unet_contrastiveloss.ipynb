{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd678b84-569f-4082-85cb-58d92f1bc85a",
   "metadata": {},
   "source": [
    "# An automated pipeline to create an atlas of in-situ hybridization gene expression data in the adult marmoset brain (ISBI 2023, Poon, C., et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a442d-ce7d-4c9a-ba61-5a7eefb85c6c",
   "metadata": {},
   "source": [
    "# UNet based semantic segmentation model with contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872d3fa-e73d-4357-89cd-bca451f2886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 20221027"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4358a-ec6e-4f7c-a89d-2b07f38e6c72",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c6a88-c045-4121-a273-fea55a2da077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms.functional as tf\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67554a77-9060-422a-aaee-7501da217c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3dc3f-093e-493b-8da3-8077a751e34d",
   "metadata": {},
   "source": [
    "### Encoder and decoder blocks of UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea70382-d50a-4ff7-ac6c-a1fcb1210827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  encoder and decoder from UNet\n",
    "\n",
    "def convbnrelu2(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "def convbnrelu2_T(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "class unet_encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16):  #64\n",
    "        super(unet_encoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_features = init_features\n",
    "        \n",
    "        self.conv_down1 = convbnrelu2(in_channels, init_features)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # not sure if this is right    \n",
    "        self.conv_down2 = convbnrelu2(init_features, init_features*2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # not sure if this is right    \n",
    "        \n",
    "        self.bottleneck = convbnrelu2(init_features*2, init_features*4)  #fc per channel, kernel size=1, reduces params\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.conv_down1(x)\n",
    "        \n",
    "        enc2 = self.conv_down2(self.maxpool1(enc1))\n",
    "        bottleneck = self.bottleneck(self.maxpool2(enc2))\n",
    "        \n",
    "        return bottleneck#, enc_list\n",
    "\n",
    "class unet_decoder(nn.Module):\n",
    "    def __init__(self, init_features=16, out_channels=1):  #64\n",
    "        super(unet_decoder, self).__init__()\n",
    "    \n",
    "        #self.convT2 = nn.ConvTranspose2d(init_features*4, init_features*2, kernel_size=2, stride=2, bias=True)\n",
    "        self.convT2 = convbnrelu2_T(init_features*4, init_features*2)\n",
    "        self.conv_up2 = convbnrelu2(init_features*2, init_features*2)\n",
    "        \n",
    "        \n",
    "        #self.convT1 = nn.ConvTranspose2d(init_features*2, init_features, kernel_size=2, stride=2, bias=True)\n",
    "        self.convT1 = convbnrelu2_T(init_features*2, init_features)\n",
    "        self.conv_up1 = convbnrelu2(init_features, init_features)   \n",
    "        \n",
    "        #init_features =16\n",
    "        #out_channels=1\n",
    "        self.final_layer = nn.Conv2d(init_features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, bottleneck):\n",
    "        \n",
    "        dec2 = self.convT2(bottleneck)\n",
    "        dec2 = self.conv_up2(dec2)\n",
    "        \n",
    "        dec1 = self.convT1(dec2)\n",
    "        dec1 = self.conv_up1(dec1)\n",
    "        \n",
    "        out = self.final_layer(dec1)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a307af-7fb7-4947-a022-38af108c6418",
   "metadata": {},
   "source": [
    "### Define contrastive transformations (see SimCLR paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99adcbb-9b35-4ac5-a947-1e26c2729ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations:\n",
    "    def __init__(self, flag, n_views=2):\n",
    "        if flag == 'augm':\n",
    "            self.base_transforms = augm_transforms\n",
    "        elif flag == 'noaugm':\n",
    "            self.base_transforms = noaugm_transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]\n",
    "\n",
    "    \n",
    "augm_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=9),\n",
    "    ]\n",
    ")\n",
    "\n",
    "noaugm_transforms = transforms.Compose(\n",
    "    [\n",
    "    ]\n",
    ")    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae551b-49d3-43f8-81a4-a8017e67d07c",
   "metadata": {},
   "source": [
    "### Dataset used for both supervised and contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1bbac-6876-43c2-9366-6a8a0855ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% class ishDataset, char edit\n",
    "class ishDataset_sim(Dataset):\n",
    "    def __init__(self, root_dir: str, img_suffix: str, label_suffix: str): \n",
    "        self.root_dir = root_dir\n",
    "        self.img_path = os.path.join(root_dir, img_suffix)\n",
    "        self.label_path = os.path.join(root_dir, label_suffix)\n",
    "        self.img_fn_list = sorted(glob.glob(self.img_path+'*.jpg'))  \n",
    "        self.label_fn_list = sorted(glob.glob(self.label_path+'*.jpg')) \n",
    "        \n",
    "        self.img_list = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        self.fast = False\n",
    "        self.fast2 = True\n",
    "        self.tt = transforms.ToTensor()\n",
    "        \n",
    "        if self.fast2:\n",
    "            for idx in range(len(self.img_fn_list)):                \n",
    "                im = self.tt(Image.open(self.img_fn_list[idx]))\n",
    "                lab = self.tt(Image.open(self.label_fn_list[idx]))\n",
    "                \n",
    "                im, lab = self.transform(im, lab)\n",
    "                \n",
    "                self.img_list += [im]\n",
    "                self.label_list+= [lab]\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.img_fn_list)\n",
    "    \n",
    "    def transform(self, image, label): \n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            image, output_size=(400,400))  \n",
    "        image = tf.crop(image, i, j, h, w)\n",
    "        label = tf.crop(label, i, j, h, w)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.fast2: \n",
    "            image, label = self.img_list[idx], self.label_list[idx]\n",
    "        return image, label\n",
    "        \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022bc2-4f97-41fd-b7c4-a0fcd921773d",
   "metadata": {},
   "source": [
    "### Dataset used for contrastive learning only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa6749-d279-4347-bada-b5f47085f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ishDataset_c(Dataset):\n",
    "    def __init__(self, root_dir: str):  \n",
    "        self.img_path = root_dir\n",
    "        self.img_fn_list = sorted(glob.glob(self.img_path+'*.jpg'))  \n",
    "        self.img_list = []\n",
    "\n",
    "        self.fast = True\n",
    "        self.tt = transforms.ToTensor()\n",
    "        \n",
    "        if self.fast:\n",
    "            for idx in range(len(self.img_fn_list)):\n",
    "                im = Image.open(self.img_fn_list[idx])\n",
    "                im = self.transform(im)\n",
    "                im = self.tt(im)\n",
    "                self.img_list += [im]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fn_list)\n",
    "    \n",
    "    def transform(self, image): \n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(image,scale=(0.9, 1.5), ratio=(0.9, 1.33))  \n",
    "        output_size=(400,400)\n",
    "        image = tf.resized_crop(image, i, j, h, w, output_size)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.fast:  \n",
    "            image = self.img_list[idx]\n",
    "\n",
    "        return image\n",
    "        \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad56a90-c8b1-419f-90df-d173984c5cdd",
   "metadata": {},
   "source": [
    "### Load datasets and visualize some samples from dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca4546-0f8e-4290-8e57-a07e62eb0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsc_data = ishDataset_sim(root_dir='/home/charissa/shimogori/shimogori_adult/ish/new/',\n",
    "                          img_suffix='img_char_train4/',   \n",
    "                          label_suffix='seg_char_train/',\n",
    "                          ) \n",
    "\n",
    "test_data = ishDataset_sim(root_dir='/home/charissa/shimogori/shimogori_adult/ish/new/',\n",
    "                          img_suffix='img_char_test2/',  \n",
    "                          label_suffix='seg_char_test/',\n",
    "                          )  \n",
    "\n",
    "trainc_data = ishDataset_c(root_dir='/home/charissa/shimogori/shimogori_adult/ish/new/im_only/jpg3/')\n",
    "\n",
    "print('train_data size:',len(trainsc_data))\n",
    "trainsplit_size = int(0.7 * len(trainsc_data))\n",
    "valsplit_size = len(trainsc_data) - trainsplit_size\n",
    "print('training_size:', trainsplit_size, 'val_size:', valsplit_size)\n",
    "trainsplit_data, valsplit_data = random_split(trainsc_data, [trainsplit_size, valsplit_size])\n",
    "\n",
    "# dataloader for supervised and contrastive learning\n",
    "trainsc_dataloader = DataLoader(trainsplit_data, batch_size=8, shuffle=True, drop_last=True, num_workers=1)\n",
    "# dataloader for contrastive learning only\n",
    "trainc_dataloader = DataLoader(trainc_data, batch_size=8, shuffle=True, drop_last=True, num_workers=1)\n",
    "# validation and test dataloaders\n",
    "val_dataloader = DataLoader(valsplit_data, batch_size=8, shuffle=False, drop_last=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, drop_last=True) \n",
    "\n",
    "train_dataloaders = {\"con\": trainc_dataloader, \"sup\": trainsc_dataloader}\n",
    "\n",
    "                                \n",
    "if True:\n",
    "    x, y = next(iter(trainsc_dataloader))\n",
    "    print(x[0].shape)  \n",
    "    print(y[0].shape)\n",
    "    for s in range(4):\n",
    "        plt.subplot(221)\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.subplot(222)\n",
    "        plt.imshow(np.transpose(y[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup labels')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(trainsc_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90351da-c16c-4d65-8b91-f9fc414011c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    x, y = next(iter(val_dataloader))\n",
    "    print(x[0].shape)  # supx_s[0][0] if stay tensor\n",
    "    print(y[0].shape)\n",
    "    for s in range(4):\n",
    "        plt.subplot(221)\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.subplot(222)\n",
    "        plt.imshow(np.transpose(y[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup labels')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(val_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc5ab9-246f-47d4-9e6d-4233a39c6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    x = next(iter(trainc_dataloader))\n",
    "    print(x[0].shape)  # supx_s[0][0] if stay tensor\n",
    "    for s in range(4):\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train sup images')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(trainc_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd016d07-18b0-471e-98ff-ec1188ddf123",
   "metadata": {},
   "source": [
    "### Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf7b4-5735-406f-aa39-773d9ee8ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, bottleneck_ch, bottleneck_h, bottleneck_w, lr, temperature, weight_decay, p_conloss):\n",
    "        super(SegModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lr = lr  #5e-4\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = 8\n",
    "        self.bottleneck_ch = bottleneck_ch\n",
    "        self.bottleneck_w = bottleneck_w\n",
    "        self.bottleneck_h = bottleneck_h\n",
    "        self.p_conloss = p_conloss\n",
    "        self.counter = 0\n",
    "        self.bce = torch.nn.BCELoss()  \n",
    "    \n",
    "        self.encoder = unet_encoder(in_channels=3, out_channels=1, init_features=16)  #64\n",
    "        self.decoder = unet_decoder(init_features=16, out_channels=1)  #64\n",
    "        \n",
    "        self.trainsc_data = trainsplit_data\n",
    "        self.trainc_data = trainc_data\n",
    "        self.valset = valsplit_data\n",
    "        self.testset = test_data\n",
    "        \n",
    "        self.ct_null = ContrastiveTransformations(flag='noaugm', n_views=2)\n",
    "        self.ct = ContrastiveTransformations(flag='augm', n_views=2)\n",
    "        \n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels=64,  #1024\n",
    "            out_channels=hidden_dim*4,\n",
    "            kernel_size=[bottleneck_w, bottleneck_h],\n",
    "            stride = [bottleneck_h, bottleneck_w]\n",
    "        )\n",
    "        self.mlp = nn.Sequential( \n",
    "            self.conv2d,\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64*hidden_dim, hidden_dim), \n",
    "            nn.Linear(hidden_dim,4*2)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        trainsc_dataloader = DataLoader(self.trainsc_data, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "        trainc_dataloader = DataLoader(self.trainc_data, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "        train_dataloaders = {\"con\": trainc_dataloader, \"sup\": trainsc_dataloader}\n",
    "        return train_dataloaders  \n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, batch_size=self.batch_size, shuffle=False, drop_last=True, num_workers=1)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        bottleneck = self.encoder(x)\n",
    "        preds = self.decoder(bottleneck)\n",
    "        return (bottleneck, preds)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) :  # , batch_nb\n",
    "        \n",
    "        if not self.automatic_optimization:\n",
    "            opt = self.optimizers()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            \n",
    "        img_train, mask_train = batch[\"sup\"] \n",
    "        img_nomi = batch[\"con\"]\n",
    "        \n",
    "        print('img_train len:', len(img_train))\n",
    "        print('mask_train len:', len(mask_train))\n",
    "\n",
    "\n",
    "        # INFO NCE LOSS #         \n",
    "\n",
    "        img_train_c = self.ct(img_train)\n",
    "        mask_train_c = self.ct_null(mask_train)\n",
    "        imgcat_train_c = torch.cat(img_train_c, dim=0)\n",
    "        bottleneck_train_c, preds_train_c = self.forward(imgcat_train_c)\n",
    "\n",
    "        loss_nce_train = self.info_nce_loss(bottleneck_train_c, mode='train')\n",
    "        self.log(\"train_contr_loss\", loss_nce_train, on_epoch=True)        \n",
    "        \n",
    "        img_nomi_c = self.ct(img_nomi)\n",
    "        imgcat_nomi_c = torch.cat(img_nomi_c, dim=0)\n",
    "        bottleneck_nomi_c, _= self.forward(imgcat_nomi_c)\n",
    "        loss_nce_nomi = self.info_nce_loss(bottleneck_nomi_c, mode='train')\n",
    "        self.log(\"train_only_contrastive\", loss_nce_nomi, on_epoch=True) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # SUPERVISED LOSS #\n",
    "       \n",
    "        mask_train2 = (mask_train > 0.5).to(dtype=torch.float32)\n",
    "        bottleneck_train_s, preds_train_s = self.forward(img_train)\n",
    "        loss_sup_train = self.bce(preds_train_s, mask_train2)\n",
    "        self.log(\"train_sup_loss\", loss_sup_train, on_epoch=True)\n",
    "\n",
    "        train_loss = loss_sup_train + (self.p_conloss/10)*loss_nce_train + (self.p_conloss/100)*loss_nce_nomi\n",
    "        \n",
    "        self.log(\"train_loss\", train_loss, on_epoch=True)\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        if not self.automatic_optimization:\n",
    "            print('if not self.automatic_optimization:')\n",
    "            self.manual_backward(loss_sup_train + 0.5*loss_nce_train)\n",
    "            opt.step()\n",
    "            \n",
    "        else:\n",
    "            return {'loss' : train_loss}\n",
    "    \n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):  # \n",
    "        if self.counter % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            x,y = batch         \n",
    "            \n",
    "\n",
    "            # INFO NCE LOSS #  \n",
    "\n",
    "            x_c = self.ct(x)\n",
    "            y_c = self.ct_null(y)\n",
    "            x_cat_c = torch.cat(x_c, dim=0)\n",
    "            y_cat_c = torch.cat(y_c, dim=0)\n",
    "\n",
    "            b_c, y_hat_c = self.forward(x_cat_c)\n",
    "            #print('b_c:',b_c.shape)\n",
    "            loss_c = self.info_nce_loss(b_c, mode='train')\n",
    "            loss_cs = self.bce(y_hat_c, y_cat_c)\n",
    "            self.log(\"val_supduringcont_loss\", loss_cs, on_epoch=True)\n",
    "\n",
    "            self.log(\"val_cont_loss\", loss_c, on_epoch=True)\n",
    "            print('val_step, nce loss:', loss_c.item())\n",
    "\n",
    "\n",
    "            # SUPERVISED LOSS #\n",
    "\n",
    "            y = (y > 0.5).to(dtype=torch.float32)\n",
    "\n",
    "            b_s, y_hat_s = self.forward(x)\n",
    "            loss_s = self.bce(y_hat_s, y)\n",
    "\n",
    "            self.log(\"val_sup_loss\", loss_s, on_epoch=True)\n",
    "            \n",
    "            # visualize validation outputs\n",
    "            if True:\n",
    "                cyhats = torch.cat((y_hat_c.detach().cpu()[0,0,...], y_hat_c.detach().cpu()[1,0,...],\n",
    "                                     y_hat_c.detach().cpu()[6,0,...], y_hat_c.detach().cpu()[7,0,...]), dim=1\n",
    "                                     )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cyhats, cmap='gray')\n",
    "                plt.title('contrastive: y_hats')\n",
    "                plt.show()\n",
    "\n",
    "                cx1 = torch.cat(\n",
    "                            (x_c[0].detach().cpu()[0,0,...], x_c[0].detach().cpu()[1,0,...],\n",
    "                              x_c[0].detach().cpu()[2,0,...], x_c[0].detach().cpu()[3,0,...]),dim=1   \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cx1, cmap='gray')\n",
    "                plt.title('contrastive: x')\n",
    "                plt.show()\n",
    "\n",
    "                cy = torch.cat(\n",
    "                            (y_c[0].detach().cpu()[0,0,...], y_c[0].detach().cpu()[1,0,...],\n",
    "                             y_c[0].detach().cpu()[2,0,...], y_c[0].detach().cpu()[3,0,...]),dim=1     \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cy, cmap='gray')\n",
    "                plt.title('contrastive: y')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                syhats = torch.cat((y_hat_s.detach().cpu()[0,0,...],  y_hat_s.detach().cpu()[1,0,...], \n",
    "                                  y_hat_s.detach().cpu()[2,0,...], y_hat_s.detach().cpu()[3,0,...]), dim=1  \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(syhats, cmap='gray')\n",
    "                plt.title('supervised: y_hats')\n",
    "                plt.show()\n",
    "\n",
    "                sx = torch.cat(\n",
    "                            (x.detach().cpu()[0,0,...], x.detach().cpu()[1,0,...],\n",
    "                             x.detach().cpu()[2,0,...], x.detach().cpu()[3,0,...]),dim=1)\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(sx, cmap='gray')\n",
    "                plt.title('supervised: x')\n",
    "                plt.show()\n",
    "\n",
    "                sy = torch.cat(\n",
    "                            (y.detach().cpu()[0,0,...], y.detach().cpu()[1,0,...],\n",
    "                             y.detach().cpu()[2,0,...], y.detach().cpu()[3,0,...]),dim=1 \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(sy, cmap='gray')\n",
    "                plt.title('supervised: y')\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "    def info_nce_loss(self, bottleneck, mode):\n",
    "        # from https://theaisummer.com/simclr/\n",
    "        \n",
    "        feats = self.mlp(bottleneck)\n",
    "        mask = (~torch.eye(self.batch_size * 2, self.batch_size * 2, dtype=bool, device=self.device)).float()\n",
    "\n",
    "        ai_cos = F.cosine_similarity(feats.unsqueeze(1), feats.unsqueeze(0), dim=2)\n",
    "\n",
    "        sim_ij = torch.diag(ai_cos, self.batch_size) \n",
    "        sim_ji = torch.diag(ai_cos, -self.batch_size)  \n",
    "\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        # nominator has positive pairs only\n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        # denominator has both positive and negative pairs, but mask each element from itself (inverse identity)\n",
    "        denominator = mask * torch.exp(ai_cos / self.temperature)\n",
    "       \n",
    "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        nce_loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "        print('nce_loss:', nce_loss)  # tensor(2.4993)\n",
    "        \n",
    "        self.log(mode + \"_nce_loss\", nce_loss)\n",
    "        \n",
    "        return nce_loss\n",
    "    \n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters())\n",
    "        return [opt]#, [sch]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a8cf8-8b93-4740-9e59-c0d6e3f6de40",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5936098-fb3b-4715-9a71-7968723cb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmodel = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, temperature=.07, weight_decay=1e-4, p_conloss=1)  \n",
    "\n",
    "# initialize with pretrained model\n",
    "pretrain_dir = '/home/charissa/shimogori/shimogori_adult/segmentation/contrastive_learning/lightning_code/csvlogs/20221027_pretraining_epochs100_patch400/version_0/checkpoints/epoch=99-step=18600.ckpt'\n",
    "pre = torch.load(pretrain_dir)\n",
    "segmodel.load_state_dict(pre['state_dict'])\n",
    "\n",
    "\n",
    "ckpt_dir = '/home/charissa/shimogori/shimogori_adult/ish/ckpt/'\n",
    "if not ckpt_dir:\n",
    "    os.mkdir(ckpt_dir)\n",
    "\n",
    "os.makedirs(ckpt_dir,exist_ok=True)\n",
    "\n",
    "if True:\n",
    "    print('training with if True:')\n",
    "    trainer = pl.Trainer(logger=CSVLogger(save_dir = 'csvlogs/', name='20221031_epochs200_pretrained'),\n",
    "                         accelerator='gpu',\n",
    "                         gpus=1,\n",
    "                        devices='gpus', \n",
    "                        default_root_dir = ckpt_dir, \n",
    "                        enable_progress_bar=False, \n",
    "                        enable_model_summary=True, \n",
    "                        max_epochs=200)  \n",
    "\n",
    "    trainer.fit(segmodel)\n",
    "\n",
    "else:  # pure pytorch\n",
    "    optimizer = torch.optim.Adam(segmodel.parameters())\n",
    "    loss = torch.nn.BCELoss()\n",
    "    segmodel.cuda()\n",
    "    for a in range(10000+1):\n",
    "        patch = trainx_s[0].cuda()\n",
    "        patch_gt = trainy_s[0].cuda()\n",
    "        segmodel.zero_grad()\n",
    "        tmp,out_ = segmodel(patch)\n",
    "        prediction = torch.sigmoid(out_)\n",
    "        l = loss(prediction,patch_gt)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if True:\n",
    "            if a % 100 == 0:\n",
    "                print('a:',a)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"iter \",a,\" loss : \",l)\n",
    "                    segmodel.eval()\n",
    "                    tmp,out_ = segmodel(patch)\n",
    "                    prediction = torch.sigmoid(out_)\n",
    "                    rimg = torch.cat(\n",
    "                        (prediction.detach().cpu()[0,0,...],patch_gt.cpu()[0,0,...]),dim=1\n",
    "                    )\n",
    "                    plt.imshow(rimg)\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642f03f-15e0-48c4-b704-92e67c815da0",
   "metadata": {},
   "source": [
    "### Visualize some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc719d98-b4d6-4fbc-bb1f-441599ee3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [4.0, 4.0]\n",
    "plt.rcParams['figure.dpi'] = 70\n",
    "\n",
    "#metrics = pd.read_csv('/home/charissa/shimogori/shimogori_adult/contrastive_learning/lightning_code/csvlogs/lightning_logs/version_6/metrics.csv')\n",
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "del metrics[\"step\"]\n",
    "\n",
    "del metrics[\"train_loss_step\"]  \n",
    "\n",
    "del metrics[\"train_loss_epoch\"]\n",
    "\n",
    "del metrics['train_nce_loss']\n",
    "del metrics['train_contr_loss_step']\n",
    "del metrics['train_sup_loss_step']\n",
    "\n",
    "\n",
    "metrics.set_index(\"epoch\", inplace=True)\n",
    "display(metrics.dropna(axis=1, how=\"all\").head())\n",
    "sn.relplot(data=metrics, kind=\"line\", height=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f125f82-f94f-4eb0-87e5-bc15ee36be16",
   "metadata": {},
   "source": [
    "### Run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1b83d-cb8e-44b4-bb56-7fe230532d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "# if loading model from file\n",
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if True:\n",
    "    segmodel2 = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, temperature=.07, weight_decay=1e-4, p_conloss=1)  # preds_h,w = 6 for patch64,96, 3 for patch48\n",
    "    segmodel2.load_state_dict(torch.load('/home/charissa/shimogori/shimogori_adult/segmentation/contrastive_learning/lightning_code/csvlogs/20221204_epochs500_2loaders_wpretrain/version_0/checkpoints/epoch=499-step=778500.ckpt')['state_dict'])  #ish_pth/20220219_unet3lvl_crop64_best_metric_model_91.pth'))\n",
    "    #segmodel2.load_state_dict(torch.load('/home/charissa/shimogori/shimogori_adult/segmentation/contrastive_learning/lightning_code/csvlogs/20221031_epochs200_lsuplcont1_2pretrainloaders/version_0/checkpoints/epoch=199-step=34000.ckpt'))\n",
    "    #segmodel2.eval()\n",
    "    segmodel2.freeze()\n",
    "    segmodel2.eval()\n",
    "\n",
    "for cmp in cmp_ls: \n",
    "    test_dir = '/home/charissa/shimogori/shimogori_adult/ish/new/img_char_test2/'\n",
    "    odir = test_dir.replace('gene','gene_seg')\n",
    "    os.makedirs(odir, exist_ok=True) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(test_f)):\n",
    "            unique_name = os.path.split(test_f[j])[1]\n",
    "\n",
    "            nibobj = nib.load(test_f[j])\n",
    "            im = nibobj.get_fdata()\n",
    "\n",
    "            im_t = tf.to_tensor(im)  \n",
    "            im_t2 = torch.unsqueeze(im_t,0)\n",
    "            _, pred = segmodel2(im_t2.float())\n",
    "\n",
    "            pred_ = pred[0,0,:,:].numpy()\n",
    "            im_out2 = rescale_intensity(pred_, in_range='image',out_range='uint8').astype(np.uint8) \n",
    "\n",
    "            print(out_name)\n",
    "            \n",
    "            new_im = nib.Nifti1Image(im_out2, nibobj.affine, nibobj.header)\n",
    "            nib.save(new_im, out_name)\n",
    "\n",
    "            if j % 10 == 0:\n",
    "                plt.imshow(im_out2, cmap='gray')\n",
    "                plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterhub] *",
   "language": "python",
   "name": "conda-env-jupyterhub-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
