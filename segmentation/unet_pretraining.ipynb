{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcaec4d-6911-4bfc-874b-8a9d0eedab45",
   "metadata": {},
   "source": [
    "# An automated pipeline to create an atlas of in-situ hybridization gene expression data in the adult marmoset brain (ISBI 2023, Poon, C., et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951248f-d4c2-4e79-8abc-25c25db09816",
   "metadata": {},
   "source": [
    "## UNet based semantic segmentation model with contrastive loss, this is the pretraining model which is only trained using the contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fc991-59ce-45d8-8239-b209ee5b3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on 20221027"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eda15b-5dea-4652-9e74-d91598e00280",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c6a88-c045-4121-a273-fea55a2da077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms.functional as tf\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67554a77-9060-422a-aaee-7501da217c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.version.cuda)\n",
    "\n",
    "#print(torch.cuda.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c7669-7216-433a-b323-be079e2547cf",
   "metadata": {},
   "source": [
    "### Encoder and decoder blocks of UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea70382-d50a-4ff7-ac6c-a1fcb1210827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  encoder and decoder from UNet\n",
    "\n",
    "def convbnrelu2(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same', bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "def convbnrelu2_T(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(), \n",
    "    )\n",
    "\n",
    "class unet_encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16): \n",
    "        super(unet_encoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.init_features = init_features\n",
    "        \n",
    "        self.conv_down1 = convbnrelu2(in_channels, init_features)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv_down2 = convbnrelu2(init_features, init_features*2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)    \n",
    "        \n",
    "        self.bottleneck = convbnrelu2(init_features*2, init_features*4)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.conv_down1(x)\n",
    "        \n",
    "        enc2 = self.conv_down2(self.maxpool1(enc1))\n",
    "        bottleneck = self.bottleneck(self.maxpool2(enc2))\n",
    "        \n",
    "        return bottleneck#, enc_list\n",
    "\n",
    "class unet_decoder(nn.Module):\n",
    "    def __init__(self, init_features=16, out_channels=1):  #64\n",
    "        super(unet_decoder, self).__init__()\n",
    "    \n",
    "        self.convT2 = convbnrelu2_T(init_features*4, init_features*2)\n",
    "        self.conv_up2 = convbnrelu2(init_features*2, init_features*2)\n",
    "        \n",
    "        self.convT1 = convbnrelu2_T(init_features*2, init_features)\n",
    "        self.conv_up1 = convbnrelu2(init_features, init_features)   \n",
    "        \n",
    "        self.final_layer = nn.Conv2d(init_features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, bottleneck):\n",
    "        \n",
    "        dec2 = self.convT2(bottleneck)\n",
    "        dec2 = self.conv_up2(dec2)\n",
    "        \n",
    "        dec1 = self.convT1(dec2)\n",
    "        dec1 = self.conv_up1(dec1)\n",
    "        \n",
    "        out = self.final_layer(dec1)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459015e-75b5-4036-99f9-cdcfab74d9ba",
   "metadata": {},
   "source": [
    "### Define contrastive transformations (see SimCLR paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99adcbb-9b35-4ac5-a947-1e26c2729ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations:\n",
    "    def __init__(self, flag, n_views=2):  \n",
    "            self.base_transforms = augm_transforms\n",
    "        elif flag == 'noaugm':\n",
    "            self.base_transforms = noaugm_transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]\n",
    "\n",
    "    \n",
    "augm_transforms = transforms.Compose(\n",
    "    [  # only apply to image\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=9),\n",
    "    ]\n",
    ")\n",
    "\n",
    "noaugm_transforms = transforms.Compose(\n",
    "    [\n",
    "    ]\n",
    ")    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5ab1d-36ac-4c22-85b8-0355fa418f7d",
   "metadata": {},
   "source": [
    "### Dataset used for contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1bbac-6876-43c2-9366-6a8a0855ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ishDataset_c(Dataset):\n",
    "    def __init__(self, root_dir: str, img_type: str):  \n",
    "        self.img_path = root_dir\n",
    "        self.img_fn_list = sorted(glob.glob(self.img_path+'*.'+self.img_type))  \n",
    "        self.img_list = []\n",
    "\n",
    "        self.fast = True\n",
    "        self.tt = transforms.ToTensor()\n",
    "        \n",
    "        if self.fast:\n",
    "            for idx in range(len(self.img_fn_list)):\n",
    "                im = Image.open(self.img_fn_list[idx])\n",
    "                im = self.transform(im)\n",
    "                im = self.tt(im)\n",
    "                self.img_list += [im]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fn_list)\n",
    "    \n",
    "    def transform(self, image): \n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(image,scale=(0.9, 1.5), ratio=(0.9, 1.33))  \n",
    "        output_size=(400,400)\n",
    "        image = tf.resized_crop(image, i, j, h, w, output_size)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.fast:  \n",
    "            image = self.img_list[idx]\n",
    "\n",
    "        return image\n",
    "        \n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53166f9-5ce3-46ce-a902-9cf7eb1f5747",
   "metadata": {},
   "source": [
    "### Load datasets and visualize some samples from dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca4546-0f8e-4290-8e57-a07e62eb0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data is assumed to be organized in the following structure:\n",
    "\n",
    "├── root directory                        , ie root_dir      (used in pretraining)\n",
    "│   ├── image directory (training)        , ie img_suffix    (not used in pretraining)\n",
    "│   ├── label directory (training)        , ie label_suffix  (not used in pretraining)\n",
    "│   ├── image directory (testing)         , ie img_suffix    (not used in pretraining)\n",
    "│   ├── label directory (testing)         , ie label_suffix  (not used in pretraining)\n",
    "│   ├── image only directory (training)   , used in trainc_data  (used in pretraining)\n",
    "\n",
    "\n",
    "where the image directory has the path:     (not used in pretraining)\n",
    "/root_dir/img_suffix/\n",
    "\n",
    "and the label directory has the path:       (not used in pretraining)\n",
    "/root_dir/label_suffix/\n",
    "\n",
    "and the images only directory has the path: (used in pretraining)\n",
    "/root_dir/img_only\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = ishDataset_c(root_dir='/home/username/projectA/img_suffix/')\n",
    "train_c = ishDataset_c(root_dir='home/username/projectA/img_only/')\n",
    "train_all = train_dataset + train_c\n",
    "\n",
    "print('train_dataset size:',len(train_all))\n",
    "training_size = int(0.7 * len(train_all))\n",
    "val_size = len(train_all) - training_size\n",
    "print('training_size:', training_size, 'val_size:', val_size)\n",
    "train_data, val_data = random_split(train_all, [training_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True, drop_last=True, num_workers=1)\n",
    "val_dataloader = DataLoader(val_data, batch_size=8, shuffle=False, drop_last=True, num_workers=1)\n",
    "\n",
    "                                \n",
    "if True:\n",
    "    x = next(iter(train_dataloader))\n",
    "    print(x[0].shape)\n",
    "    for s in range(4):\n",
    "        plt.imshow(np.transpose(x[s], (1,2,0)), cmap='gray')\n",
    "        plt.title('train images')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"ok\")\n",
    "    print(train_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e3c86-9f03-414c-90b3-58ead7859f56",
   "metadata": {},
   "source": [
    "### Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adaf7b4-5735-406f-aa39-773d9ee8ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class SegModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, bottleneck_ch, bottleneck_h, bottleneck_w, lr, temperature, weight_decay, p_conloss, out_name):\n",
    "        super(SegModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.lr = lr\n",
    "        self.temperature = temperature\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = 8\n",
    "        self.bottleneck_ch = bottleneck_ch\n",
    "        self.bottleneck_w = bottleneck_w\n",
    "        self.bottleneck_h = bottleneck_h\n",
    "        self.counter = 0\n",
    "        self.out_name = out_name\n",
    "        self.p_conloss = p_conloss\n",
    "    \n",
    "        self.encoder = unet_encoder(in_channels=3, out_channels=1, init_features=16)  #64\n",
    "        self.decoder = unet_decoder(init_features=16, out_channels=1)  #64\n",
    "        \n",
    "        self.trainset = train_data\n",
    "        self.valset = val_data\n",
    "        \n",
    "        self.ct_null = ContrastiveTransformations(flag='noaugm', n_views=2)\n",
    "        self.ct = ContrastiveTransformations(flag='augm', n_views=2)\n",
    "        \n",
    "        # conv2d for the MLP (below)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels=64,  #1024\n",
    "            out_channels=hidden_dim*4,\n",
    "            kernel_size=[bottleneck_w, bottleneck_h],\n",
    "            stride = [bottleneck_h, bottleneck_w]\n",
    "        )\n",
    "        # MLP used for the contrastive loss\n",
    "        self.mlp = nn.Sequential( \n",
    "            self.conv2d,\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64*hidden_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim,4*2)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=1)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valset, batch_size=self.batch_size, shuffle=False, drop_last=True, num_workers=1)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        bottleneck = self.encoder(x)\n",
    "        preds = self.decoder(bottleneck)\n",
    "        return (bottleneck, preds)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) :  \n",
    "        \n",
    "        if not self.automatic_optimization:\n",
    "            opt = self.optimizers()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        img_train = batch \n",
    "        print('img_train len:', len(img_train))\n",
    "\n",
    "        \n",
    "        # INFO NCE LOSS #         \n",
    "               \n",
    "        img_train_c = self.ct(img_train)\n",
    "        imgcat_train_c = torch.cat(img_train_c, dim=0)\n",
    "        bottleneck_train_c, _ = self.forward(imgcat_train_c)\n",
    "        \n",
    "        loss_nce_train = self.info_nce_loss(bottleneck_train_c, mode='train')\n",
    "        self.log(\"train_contr_loss\", loss_nce_train, on_epoch=True)        \n",
    "        \n",
    "       \n",
    "        self.counter += 1\n",
    "\n",
    "        if not self.automatic_optimization:\n",
    "            print('if not self.automatic_optimization:')\n",
    "            self.manual_backward(loss_nce_train)\n",
    "            opt.step()\n",
    "            \n",
    "        else:\n",
    "            return {'loss' :loss_nce_train}\n",
    "    \n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):  # \n",
    "        \n",
    "        if self.counter % 5 == 0:\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            x = batch         \n",
    "            \n",
    "            # INFO NCE LOSS #  \n",
    "            x_c = self.ct(x)\n",
    "            x_cat_c = torch.cat(x_c, dim=0)\n",
    "\n",
    "            b_c, _ = self.forward(x_cat_c)\n",
    "            loss_c = self.info_nce_loss(b_c, mode='train')\n",
    "            self.log(\"val_cont_loss\", loss_c, on_epoch=True)\n",
    "\n",
    "         \n",
    "            # visualize validation outputs\n",
    "            if True:\n",
    "                cx1 = torch.cat(\n",
    "                            (x_c[0].detach().cpu()[0,0,...], x_c[0].detach().cpu()[1,0,...],\n",
    "                              x_c[0].detach().cpu()[2,0,...], x_c[0].detach().cpu()[3,0,...]),dim=1   \n",
    "                        )\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(cx1)\n",
    "                plt.title('contrastive: x')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                sx = torch.cat(\n",
    "                            (x.detach().cpu()[0,0,...], x.detach().cpu()[1,0,...],\n",
    "                             x.detach().cpu()[2,0,...], x.detach().cpu()[3,0,...]),dim=1)\n",
    "                plt.figure(figsize = (20,5))\n",
    "                plt.imshow(sx)\n",
    "                plt.title('supervised: x')\n",
    "                plt.show()\n",
    "\n",
    "        if self.counter % 20 == 0:\n",
    "            # naming\n",
    "            out_dir = mid_outpath+self.out_name+'/'\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.mkdir(out_dir)            \n",
    "            \n",
    "            out_name_root = out_dir + str(self.counter) + '_'\n",
    "            \n",
    "            cv2.imwrite((out_name_root+'con_x.png'), np.asarray(cx1*255).astype(np.uint8))\n",
    "\n",
    "        self.counter +=1\n",
    "\n",
    "        \n",
    "    def info_nce_loss(self, bottleneck, mode):\n",
    "        # from https://theaisummer.com/simclr/\n",
    "        \n",
    "        feats = self.mlp(bottleneck)\n",
    "        mask = (~torch.eye(self.batch_size * 2, self.batch_size * 2, dtype=bool, device=self.device)).float()\n",
    "\n",
    "        ai_cos = F.cosine_similarity(feats.unsqueeze(1), feats.unsqueeze(0), dim=2)\n",
    "\n",
    "        sim_ij = torch.diag(ai_cos, self.batch_size) \n",
    "        sim_ji = torch.diag(ai_cos, -self.batch_size)  \n",
    "\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        # nominator has positive pairs only\n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        # denominator has both positive and negative pairs, but mask each element from itself (inverse identity)\n",
    "        denominator = mask * torch.exp(ai_cos / self.temperature)\n",
    "       \n",
    "        all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        nce_loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "        print('nce_loss:', nce_loss)  \n",
    "        \n",
    "        self.log(mode + \"_nce_loss\", nce_loss)\n",
    "        \n",
    "        return nce_loss\n",
    "    \n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters())\n",
    "        return [opt]#, [sch]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383b221-e089-48b2-954b-054cd1151a05",
   "metadata": {},
   "source": [
    "### Run pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5936098-fb3b-4715-9a71-7968723cb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create name for saved model\n",
    "date = 20221027\n",
    "max_epochs = 100\n",
    "p_conloss = 1\n",
    "out_name=str(date)+'_pretraining_epochs'+str(max_epochs)+'_patch400'\n",
    "\n",
    "segmodel = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, temperature=.07, weight_decay=1e-4, p_conloss=p_conloss, out_name=out_name)\n",
    "\n",
    "# define checkpoint directory\n",
    "ckpt_dir = '/home/username/projectA/ckpt/'\n",
    "os.makedirs(ckpt_dir,exist_ok=True)\n",
    "\n",
    "if True:\n",
    "    print('training with if True:')\n",
    "    trainer = pl.Trainer(logger=CSVLogger(save_dir = 'csvlogs/', name=out_name),\n",
    "                         accelerator=\"gpu\",\n",
    "                         gpus=1,\n",
    "                         devices='gpus',\n",
    "                         default_root_dir = ckpt_dir,\n",
    "                         enable_progress_bar=True,\n",
    "                         enable_model_summary=True,\n",
    "                         max_epochs=max_epochs)  \n",
    "\n",
    "    trainer.fit(segmodel)\n",
    "\n",
    "else:  # pure pytorch\n",
    "    x,y = next(iter(train_dataloader))\n",
    "    optimizer = torch.optim.Adam(segmodel.parameters())\n",
    "    loss = torch.nn.BCELoss()\n",
    "    segmodel.cuda()\n",
    "    for a in range(10000+1):\n",
    "        patch = x.cuda()\n",
    "        patch_gt = y.cuda()\n",
    "        segmodel.zero_grad()\n",
    "        tmp,out_ = segmodel(patch)\n",
    "        prediction = torch.sigmoid(out_)\n",
    "        l = loss(prediction,patch_gt)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if True:\n",
    "            if a % 100 == 0:\n",
    "                print('a:',a)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"iter \",a,\" loss : \",l)\n",
    "                    segmodel.eval()\n",
    "                    tmp,out_ = segmodel(patch)\n",
    "                    prediction = torch.sigmoid(out_)\n",
    "                    rimg = torch.cat(\n",
    "                        (prediction.detach().cpu()[0,0,...],patch_gt.cpu()[0,0,...]),dim=1\n",
    "                    )\n",
    "                    plt.imshow(rimg)\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ddaa6a-1e96-45c7-85b2-c451adeab2d2",
   "metadata": {},
   "source": [
    "### Visualize some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc719d98-b4d6-4fbc-bb1f-441599ee3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [4.0, 4.0]\n",
    "plt.rcParams['figure.dpi'] = 70\n",
    "\n",
    "#metrics = pd.read_csv('/home/charissa/shimogori/shimogori_adult/contrastive_learning/lightning_code/csvlogs/lightning_logs/version_6/metrics.csv')\n",
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "del metrics[\"step\"]\n",
    "del metrics[\"train_loss_step\"]  \n",
    "del metrics[\"train_loss_epoch\"]\n",
    "del metrics['train_nce_loss']\n",
    "del metrics['train_contr_loss_step']\n",
    "del metrics['train_sup_loss_step']\n",
    "\n",
    "metrics.set_index(\"epoch\", inplace=True)\n",
    "display(metrics.dropna(axis=1, how=\"all\").head())\n",
    "sn.relplot(data=metrics, kind=\"line\", height=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6733b4-5717-42d2-ae54-ff9cf2a754a7",
   "metadata": {},
   "source": [
    "### Visualize some results from pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52faff-9f35-452f-a62e-52dd4f053ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to load a different, saved model\n",
    "segmodel = SegModel(hidden_dim=32, bottleneck_ch=64, bottleneck_h=24, bottleneck_w=24, lr=1e-3, \n",
    "                    temperature=.07, weight_decay=1e-4, p_conloss=1, out_name='meow')  # preds_h,w = 6 for patch64,96, 3 for patch48\n",
    "    \n",
    "ckpt_path = '/home/charissa/shimogori/shimogori_adult/segmentation/contrastive_learning/lightning_code/csvlogs/202210124_pretraining_epochs500/version_0/checkpoints/epoch=499-step=177000_copy.ckpt'\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "segmodel.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a4f33-de7a-489c-a9db-f1d0f3aa5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8.0, 8.0]\n",
    "plt.rcParams['figure.dpi'] = 140\n",
    "\n",
    "t_invert = transforms.RandomInvert(p=0.5)\n",
    "\n",
    "segmodel.eval()\n",
    "with torch.no_grad():\n",
    "    x,y = next(iter(train_dataloader))\n",
    "    x = t_invert(x)\n",
    "    bottleneck, val_preds = segmodel(x) \n",
    "    print(x[0,0,...].shape)    \n",
    "    print(y[0,0,...].shape)\n",
    "    print(val_preds[0,0,...].shape)\n",
    "\n",
    "\n",
    "    frame1 = torch.cat((x.detach().cpu()[0,0,...],val_preds.detach().cpu()[0,0,...],y.cpu()[0,0,...]),dim=1)\n",
    "    plt.imshow(frame1, cmap='gray')\n",
    "    plt.title('x, preds, y')\n",
    "    plt.show()\n",
    "    frame2 = torch.cat((x.detach().cpu()[1,0,...],val_preds.detach().cpu()[1,0,...],y.cpu()[1,0,...]),dim=1)\n",
    "    plt.title('x, preds, y')\n",
    "    plt.imshow(frame2, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    print('x[0,0,...]:',x[0,0,...].min(), x[0,0,...].max(), x[0,0,...].dtype)\n",
    "    print('y.cpu()[0,0,...]:',y.cpu()[0,0,...].min(), y.cpu()[0,0,...].max(), y.cpu()[0,0,...].dtype)\n",
    "    print('val_preds:',val_preds.detach().cpu()[0,0,...].min(), val_preds.detach().cpu()[0,0,...].max(), val_preds.detach().cpu()[0,0,...].dtype)\n",
    "    assert x[0,0,...].min() >= 0   \n",
    "    assert x[0,0,...].max() >= 0 \n",
    "    assert x[0,0,...].max() <= 1 \n",
    "    assert y[0,0,...].min() >= 0 \n",
    "    assert y[0,0,...].max() <= 1 \n",
    "    assert val_preds[0,0,...].min() >= 0 \n",
    "    assert val_preds[0,0,...].max() <= 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b05f3e-d55e-4405-8eec-04d33157b918",
   "metadata": {},
   "source": [
    "### If you would like to count some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bd0b7-b03f-46a6-af8b-72e2014f1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/charissa/shimogori/shimogori_adult/contrastive_learning/lightning_code/csvlogs/lsup_only/version_3/checkpoints/epoch=49-step=4950.ckpt'\n",
    "segmodel2 = SegModel.load_from_checkpoint(PATH).cpu()\n",
    "segmodel2.eval()\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in segmodel2.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterhub] *",
   "language": "python",
   "name": "conda-env-jupyterhub-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
